{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./gym_dagsched/data_generation/tpch/')\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "from gym_dagsched.envs.dagsched_sim import DagSchedSim\n",
    "from gym_dagsched.policies.decima_agent import ActorNetwork\n",
    "from gym_dagsched.utils.metrics import avg_job_duration, makespan\n",
    "from gym_dagsched.data_generation.random_datagen import RandomDataGen\n",
    "from gym_dagsched.data_generation.tpch_datagen import TPCHDataGen\n",
    "from visualization import make_gantt\n",
    "\n",
    "\n",
    "datagen = RandomDataGen(\n",
    "    max_ops=20,\n",
    "    # max_tasks=200,\n",
    "    max_tasks=4,\n",
    "    mean_task_duration=2000.,\n",
    "    n_worker_types=1)\n",
    "\n",
    "# datagen = TPCHDataGen()\n",
    "\n",
    "sim = DagSchedSim()\n",
    "policy = ActorNetwork(5, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_op(op_idx):\n",
    "    i = 0\n",
    "    for job in sim.jobs:\n",
    "        if op_idx < i + len(job.ops):\n",
    "            op = job.ops[op_idx - i]\n",
    "            break\n",
    "        else:\n",
    "            i += len(job.ops)\n",
    "    return op\n",
    "\n",
    "def sample_action(ops, prlvl):\n",
    "    c = torch.distributions.Categorical(probs=ops)        \n",
    "    next_op_idx = c.sample().item()\n",
    "    next_op = find_op(next_op_idx)\n",
    "    c = torch.distributions.Categorical(probs=prlvl[next_op.job_id])        \n",
    "    n_workers = c.sample().item()\n",
    "\n",
    "    return next_op_idx, next_op, n_workers\n",
    "\n",
    "\n",
    "def run_episode(ep_length, initial_timeline, workers):\n",
    "    sim.reset(initial_timeline, workers)\n",
    "    actions = []\n",
    "    obsns = []\n",
    "    rewards = []\n",
    "    # done = False\n",
    "\n",
    "    while len(actions) < ep_length:\n",
    "        dags = []\n",
    "        op_msk = []\n",
    "        for job in sim.jobs:\n",
    "            job.update_feature_vectors(sim.workers)\n",
    "            dags += [from_networkx(job.dag)]\n",
    "            for op in job.ops:\n",
    "                op_msk += [1] if op in sim.frontier_ops else [0]\n",
    "\n",
    "        if len(dags) == 0:\n",
    "            sim.step(None, 0)\n",
    "            continue\n",
    "\n",
    "        dag_batch = Batch.from_data_list(dags)\n",
    "        op_msk = torch.tensor(op_msk)\n",
    "        prlvl_msk = torch.ones((len(dags), len(sim.workers)))\n",
    "        obsns += [(dag_batch, len(sim.workers), op_msk, prlvl_msk)]\n",
    "        \n",
    "        ops, prlvl = policy(dag_batch, len(sim.workers), op_msk, prlvl_msk)\n",
    "        next_op_idx, next_op, n_workers = sample_action(ops, prlvl)\n",
    "        actions += [(next_op_idx, n_workers)]\n",
    "        \n",
    "        _, reward = sim.step(next_op, n_workers)\n",
    "        rewards += [reward]\n",
    "    \n",
    "    return actions, obsns, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.60151029415431\n",
      "91.60151029415431\n",
      "91.60151029415431\n",
      "284.4567625895606\n",
      "512.4832229539913\n",
      "1479.8348003238366\n",
      "2469.542329704819\n",
      "2487.211906754563\n",
      "2599.5098880106207\n",
      "2932.2596829992403\n",
      "4195.065597488427\n",
      "4960.611085366908\n",
      "5140.7898871052785\n",
      "5520.64946434274\n",
      "6644.868459636693\n",
      "6779.74795637066\n",
      "8330.492973746606\n",
      "8342.748809291334\n",
      "8608.751630616796\n",
      "9129.228533501575\n",
      "9645.379506806634\n",
      "9645.379506806634\n",
      "9670.458909713563\n",
      "13152.992847729982\n",
      "13498.227033722464\n",
      "13586.187348616\n",
      "14439.245603634014\n",
      "14442.447643649008\n",
      "14535.072268430125\n",
      "14976.154764451943\n",
      "14988.984906760736\n",
      "15476.765221956904\n",
      "16140.446344821014\n",
      "16433.77667710228\n",
      "17068.461175721084\n",
      "18749.203524225133\n",
      "18788.11831486273\n",
      "18973.129714328195\n",
      "19367.57347106927\n",
      "21094.397737874988\n",
      "21094.397737874988\n",
      "21577.082285168355\n",
      "24182.854852698358\n",
      "24182.854852698358\n",
      "24182.854852698358\n",
      "24778.09315658224\n",
      "24778.09315658224\n",
      "24778.09315658224\n",
      "25075.303763122265\n",
      "27025.86741360771\n",
      "27341.054332917716\n",
      "27977.076839537727\n",
      "29227.059543987194\n",
      "30104.6469579167\n",
      "30176.10951638595\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_SEQUENCES = 1\n",
    "N_EP_PER_SEQ = 1\n",
    "mean_ep_length = 20\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=.005)\n",
    "\n",
    "for _ in range(N_SEQUENCES):\n",
    "    ep_length = np.random.geometric(1/mean_ep_length)\n",
    "\n",
    "    initial_timeline = datagen.initial_timeline(\n",
    "        n_job_arrivals=20, n_init_jobs=0, mjit=1000.)\n",
    "    workers = datagen.workers(n_workers=5)\n",
    "\n",
    "    episodes = []\n",
    "    for _ in range(N_EP_PER_SEQ):\n",
    "        actions, obsns, rewards = run_episode(ep_length, initial_timeline, workers)\n",
    "        total_rewards = []\n",
    "        for i in range(len(rewards)):\n",
    "            total_rewards += [sum(rewards[i:])]\n",
    "        episodes += [(actions, obsns, total_rewards)]\n",
    "\n",
    "    for k in range(ep_length):\n",
    "        baseline = np.mean([episode[2][k] for episode in episodes])\n",
    "        for actions, obsns, total_reward in episodes:\n",
    "            action, obs, total_reward = actions[k], obsns[k], total_reward[k]\n",
    "            dag_batch, len_workers, op_msk, prlvl_msk = obs\n",
    "            next_op_idx, n_workers = action\n",
    "\n",
    "            ops, prlvl = policy(dag_batch, len_workers, op_msk, prlvl_msk)\n",
    "\n",
    "            c = torch.distributions.Categorical(probs=ops)\n",
    "            next_op_lgp = c.log_prob(torch.tensor(next_op_idx))\n",
    "            \n",
    "            next_op = find_op(next_op_idx)\n",
    "            c = torch.distributions.Categorical(probs=prlvl[next_op.job_id])  \n",
    "            n_workers_lgp = c.log_prob(torch.tensor(n_workers))\n",
    "\n",
    "            loss = -(next_op_lgp + n_workers_lgp) * (total_reward - baseline)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "    mean_ep_length += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-44.63120231528518,\n",
       " -44.63120231528518,\n",
       " -44.63120231528518,\n",
       " -44.611916790055645,\n",
       " -44.56631149798275,\n",
       " -44.276106024771806,\n",
       " -43.8802230130194,\n",
       " -43.87138822449454,\n",
       " -43.804009435740895,\n",
       " -43.60435955874773,\n",
       " -42.84667601005421,\n",
       " -42.310794168539275,\n",
       " -42.16665112714857,\n",
       " -41.8627634653586,\n",
       " -40.85096636959405,\n",
       " -40.716086872860075,\n",
       " -39.16534185548413,\n",
       " -39.15308601993941,\n",
       " -38.88708319861394,\n",
       " -38.31455860544068,\n",
       " -37.69517743747461,\n",
       " -37.69517743747461,\n",
       " -37.66508215398629,\n",
       " -33.13778803456495,\n",
       " -32.654460174175476,\n",
       " -32.531315733324526,\n",
       " -31.251728350797507,\n",
       " -31.246605086773513,\n",
       " -31.098405687123723,\n",
       " -30.348565443886635,\n",
       " -30.325471187730813,\n",
       " -29.398688588858086,\n",
       " -28.07132634312987,\n",
       " -27.484665678567335,\n",
       " -26.21529668132973,\n",
       " -22.853811984321634,\n",
       " -22.775982403046438,\n",
       " -22.40595960411551,\n",
       " -21.61707209063336,\n",
       " -18.163423557021925,\n",
       " -18.163423557021925,\n",
       " -17.19805446243519,\n",
       " -11.986509327375186,\n",
       " -11.986509327375186,\n",
       " -11.986509327375186,\n",
       " -10.796032719607423,\n",
       " -10.796032719607423,\n",
       " -10.796032719607423,\n",
       " -10.201611506527373,\n",
       " -6.30048420555648,\n",
       " -5.670110366936467,\n",
       " -4.398065353696445,\n",
       " -1.8980999447975118,\n",
       " -0.1429251169385025]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "868d701682088d1128cad788f3b783d362f359ccbd74a171330d82ee9fcc2b54"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('archienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
