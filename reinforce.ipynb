{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./gym_dagsched/data_generation/tpch/')\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "from gym_dagsched.envs.dagsched_env import DagSchedEnv\n",
    "from gym_dagsched.policies.decima_agent import ActorNetwork\n",
    "from gym_dagsched.utils.metrics import avg_job_duration, makespan\n",
    "from gym_dagsched.data_generation.random_datagen import RandomDataGen\n",
    "from gym_dagsched.data_generation.tpch_datagen import TPCHDataGen\n",
    "from visualization import make_gantt\n",
    "\n",
    "\n",
    "datagen = RandomDataGen(\n",
    "    max_ops=20,\n",
    "    # max_tasks=200,\n",
    "    max_tasks=4,\n",
    "    mean_task_duration=2000.,\n",
    "    n_worker_types=1)\n",
    "\n",
    "# datagen = TPCHDataGen()\n",
    "\n",
    "n_workers = 5\n",
    "sim = DagSchedEnv()\n",
    "policy = ActorNetwork(5, 8, n_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_op(op_idx):\n",
    "    i = 0\n",
    "    for job in sim.jobs:\n",
    "        if op_idx < i + len(job.ops):\n",
    "            op = job.ops[op_idx - i]\n",
    "            break\n",
    "        else:\n",
    "            i += len(job.ops)\n",
    "    return op\n",
    "\n",
    "def sample_action(ops, prlvl):\n",
    "    c = torch.distributions.Categorical(probs=ops)        \n",
    "    next_op_idx = c.sample().item()\n",
    "    next_op = find_op(next_op_idx)\n",
    "    c = torch.distributions.Categorical(probs=prlvl[next_op.job_id])        \n",
    "    n_workers = c.sample().item()\n",
    "\n",
    "    return next_op_idx, next_op, n_workers\n",
    "\n",
    "\n",
    "def run_episode(ep_length, initial_timeline, workers):\n",
    "    sim.reset(initial_timeline, workers)\n",
    "    \n",
    "    actions = []\n",
    "    obsns = []\n",
    "    rewards = []\n",
    "\n",
    "    done = False\n",
    "    obs = None\n",
    "\n",
    "    while len(actions) < ep_length and not done:\n",
    "        if obs is None:\n",
    "            next_op, n_workers = None, 0\n",
    "        else:\n",
    "            dag_batch, op_msk, prlvl_msk = obs\n",
    "            ops, prlvl = policy(dag_batch, op_msk, prlvl_msk)\n",
    "            next_op_idx, next_op, n_workers = sample_action(ops, prlvl)\n",
    "            \n",
    "            actions += [(next_op_idx, n_workers)]\n",
    "            obsns += [obs]\n",
    "            rewards += [reward]\n",
    "\n",
    "        obs, reward, done = sim.step(next_op, n_workers)\n",
    "    \n",
    "    return actions, obsns, rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of job arrival sequences to train on\n",
    "N_SEQUENCES = 1\n",
    "\n",
    "# number of times to train on a fixed sequence\n",
    "N_EP_PER_SEQ = 1\n",
    "\n",
    "# (geometric distribution) mean number of environment \n",
    "# steps in an episode; this quantity gradually increases\n",
    "# as a form of curriculum learning\n",
    "mean_ep_length = 20\n",
    "\n",
    "# Adam optimizer for training the actor network\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=.005)\n",
    "\n",
    "\n",
    "\n",
    "def compute_action_log_probs(ops, prlvl, next_op_idx, n_workers):\n",
    "    c = torch.distributions.Categorical(probs=ops)\n",
    "    next_op_idx_lgp = c.log_prob(torch.tensor(next_op_idx))\n",
    "    \n",
    "    next_op = find_op(next_op_idx)\n",
    "    c = torch.distributions.Categorical(probs=prlvl[next_op.job_id])  \n",
    "    n_workers_lgp = c.log_prob(torch.tensor(n_workers))\n",
    "\n",
    "    return next_op_idx_lgp, n_workers_lgp\n",
    "\n",
    "\n",
    "\n",
    "def run_episodes(initial_timeline, workers, ep_length):\n",
    "    # run multiple episodes on the same sequence, and\n",
    "    # records the (action,observation,reward) histories \n",
    "    # of each episode. Each of the following list/array\n",
    "    # objects will have shape (N_EP_PER_SEQ, ep_length)\n",
    "    actions_histories = []\n",
    "    obsns_histories = []\n",
    "    total_rewards_histories = np.zeros((N_EP_PER_SEQ, ep_length))\n",
    "\n",
    "    for i in range(N_EP_PER_SEQ):\n",
    "        actions, obsns, rewards = run_episode(ep_length, initial_timeline, workers)\n",
    "        rewards = np.array(rewards)\n",
    "        total_rewards = np.cumsum(rewards[::-1])[::-1]\n",
    "        actions_histories += [actions]\n",
    "        obsns_histories += [obsns]\n",
    "        total_rewards_histories[i] = total_rewards\n",
    "\n",
    "    return actions_histories, obsns_histories, total_rewards_histories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(N_SEQUENCES):\n",
    "    ep_length = np.random.geometric(1/mean_ep_length)\n",
    "\n",
    "    # sample a job arrival sequence and worker types\n",
    "    initial_timeline = datagen.initial_timeline(\n",
    "        n_job_arrivals=20, n_init_jobs=0, mjit=1000.)\n",
    "    workers = datagen.workers(n_workers=n_workers)\n",
    "\n",
    "     # run multiple episodes on this fixed sequence\n",
    "    actions_histories, obsns_histories, total_rewards_histories = \\\n",
    "        run_episodes(initial_timeline, workers, ep_length)\n",
    "\n",
    "    for k in range(ep_length):\n",
    "        baseline = total_rewards_histories[:,k].mean()\n",
    "        zip_histories = zip(actions_histories, obsns_histories, total_rewards_histories)\n",
    "        for actions, obsns, total_rewards in zip_histories:\n",
    "            action, obs, total_reward = actions[k], obsns[k], total_rewards[k]\n",
    "            dag_batch, op_msk, prlvl_msk = obs\n",
    "            next_op_idx, n_workers = action\n",
    "\n",
    "            ops, prlvl = policy(dag_batch, op_msk, prlvl_msk)\n",
    "\n",
    "            next_op_idx_lgp, n_workers_lgp = \\\n",
    "                compute_action_log_probs(ops, prlvl, next_op_idx, n_workers)\n",
    "                \n",
    "            loss = -(next_op_idx_lgp + n_workers_lgp) * (total_reward - baseline)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            \n",
    "    mean_ep_length += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16883803, -0.16883803, -0.16883803, -0.16883803, -0.02042514,\n",
       "        -0.00030304]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "868d701682088d1128cad788f3b783d362f359ccbd74a171330d82ee9fcc2b54"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('archienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
